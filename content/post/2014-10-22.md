---
title: 博客全站下载备份（含评论）
author: 大鹏
summary: "以前我写过一篇题为[在 Kindle 上读博客][1] 的帖子，讲的是如何把一些独立博客全站的博文扒下来，整理后放到 Kindle 上阅读的方法。用这个方法，我扒下来好几个精彩的独立博客，在 Kindle 上着实过了一把博客瘾。然而，过完瘾后发现了一个问题：别人的可以，自己的博客反倒是扒不下来了。而本文介绍的方法，可以通杀全部含有归档页的网络日志。"
type: post
date: 2014-10-21T16:43:55+00:00
url: /archives/17635
views:
  - 62
categories:
  - 未分类
tags:
  - R
  - wordpress

---
以前我写过一篇题为[在 Kindle 上读博客][1] 的帖子，讲的是如何把一些独立博客全站的博文扒下来，整理后放到 Kindle 上阅读的方法。用这个方法，我扒下来好几个精彩的独立博客，在 Kindle 上着实过了一把博客瘾。然而，过完瘾后发现了一个问题：别人的可以，自己的博客反倒是扒不下来了。而本文介绍的方法，可以通杀全部含有归档页的网络日志。

仔细研究了一下，原来，能扒下来的博客，归档的帖子地址都有规律可循，比如土木坛子和博士牌民工的归档地址规律都是 http://yourdomain.com/archives/i，其中i是个自然数。这是博主在wordpress后台这样设置的。因此，[用 wget 下载][1]时设置个循环让 i 递增就可以了。而我的博客，归档地址是按帖子标题来的，比如 “http://pzhao.org/2013-04-02-深情相拥（吉他谱）”这个帖子，地址中的汉字被自动转换成了乱七八糟的字符：

    http://pzhao.org/2013-04-02-%e6%b7%b1%e6%83%85%e7%9b%b8%e6%8b%a5%ef%bc%88%e5%90%89%e4%bb%96%e8%b0%b1%ef%bc%89/
    

难怪扒不下来啊！如果别人的博客也是这样，那该怎么下载呢？

在万能的R面前，这是小菜一碟。思路跟前两天写的<[批量下载网页上的图片][2]>一文完全相同：从博客的归档页中获取全部博文的链接，对这些链接的文本进行处理，删除冗余信息，下载所需的博文。

博客一般都有个归档页，显示了全部博文的目录。“大鹏志”的归档页地址是[http://pzhao.org/contents][3]，所有带链接的博文都在这里。在 R 中利用 readLines() 函数读取这个页面。查看得到的文本，发现里边链接很多，要下载链接的文本行其规律是以“`\t\t\t<li>`”开头的。这就好办了，只要用正则表达式挑出这些链接就行了。这一步很关键。后面的步骤就容易了。

下载备份“大鹏志”全部博文（含评论）到本地 e:\dapengde 文件夹的 R 代码如下：

    dir.create('d:\\dapengde')
    setwd('d:\\dapengde')
    aa <- readLines('http://pzhao.org/contents/', encoding='UTF-8') # 读取网页
    for (i in 1:length(aa)) # 逐行处理
    {
    print(i)
      if (regexpr('\t\t\t<li>', aa[i])[1] == 1) # 挑出需要的链接行
      {
        link <- substring(aa[i], regexpr("http", aa[i])[1], regexpr("\" >", aa[i])[1]-2) # 获取链接
        output <- paste(i, ".html", sep = "") # 指定文件名
        download.file(link, output) # 下载网页
      }
    }
    

如果会使用 vim 编辑器，那么不必用 R， 只需将网页源码在 vim 中用正则表达式处理一下，修改成 wget 命令下载网页的批处理文件，也可以得到同样的效果。

利用这种方法，可以通吃所有含有归档页的博客，将其全文和评论下载备份到本地电脑中。

注：本文写于2014-01-01。不知怎的，忘在了草稿箱里，今天才发现，略作改动后发表。

 [1]: http://pzhao.org/2013-03-05/
 [2]: http://pzhao.org/archives/17610
 [3]: http://pzhao.org/contents/
