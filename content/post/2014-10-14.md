---
title: 批量下载网页上的图片
author: 大鹏
summary: "[新工作单位的网页][1]做得简洁大方，ctrl+u一看网页源代码，发现竟然是拿wordpress建的。网站下面有个[图片展示页面][2]，展示的是我们这个研究组的野外观测照片缩略图，按站点分了类。作为一个新来的，自然要把本组的工作了解一番；看图的时候，需要逐个点开才能看大图，不如下载到本地方便。但是这几百张图，一一点开下载也太累了。如何批量下载网页上的图片呢？"
type: post
date: 2014-10-14T09:48:03+00:00
url: /archives/17610
views:
  - 99
categories:
  - 未分类
tags:
  - R
  - wordpress

---
[新工作单位的网页][1]做得简洁大方，ctrl+u一看网页源代码，发现竟然是拿wordpress建的。网站下面有个[图片展示页面][2]，展示的是我们这个研究组的野外观测照片缩略图，按站点分了类。作为一个新来的，自然要把本组的工作了解一番；看图的时候，需要逐个点开才能看大图，不如下载到本地方便。但是这几百张图，一一点开下载也太累了。如何批量下载网页上的图片呢？

方案有很多，列举如下：

  1. 可以通过浏览器的&#8217;保存网页全部内容&#8217;来实现，本地生成一个文件夹，包含了网页上的图片。这个我试了，但只保存下来了缩略图，没有大图。 
  2. 可以安装迅雷、快车之类的软件，但是我不想装。有些软件臃肿庞大也就算了，关键是不知道他们在背后悄悄做了些什么。另外，他们无法解决图片分类的问题，详见第4条。
  3. 傲游、360等浏览器有批量下载功能，或者firefox+BatchDownload 插件也行，但我不想装，并且他们也无法解决图片分类的问题。我只用chrome。
  4. chrome有个fatkun插件，专门用来批量下载图片，能下载大图，是我最满意的方案了，但也并非完美。下载的图片文件名要么是原始文件名，要么只能简单编号。这样一来，所有观测站点的图片都混在了一起，这仍然不是我想要的。我希望下载到本地的图片能自动按观测站分类保存。

其实，每张图片所属站点的信息，包含在了图片的链接里。比如Neustift观测站某图的链接是:

    http://www.biomet.co.at/wp/wp-content/gallery/neustift/img_8260_59_58_tonemapped_a1_w3000px.jpg
    

这个链接里是含有站名信息的。这就好办了，可以自己动手用代码实现。

R代码好久不写，手都生了。下面是我写的R代码，先是从网页源代码里获取图片的下载链接，然后从链接中截取站名，并为下载到本地的图片重命名，只需要十几行。这个代码比较笨，如果我还能记得R里的正则表达式，就可以写得更漂亮些。

    dir.create('d:\\dapengde')
    setwd('d:\\dapengde')
    linkformat <- 'src="http://www.biomet.co.at/wp/wp-content/gallery'
    urlink <- 'http://www.biomet.co.at/pictures/'
    aa <- readLines(urlink, encoding='UTF-8') # 读取网页
    bb <- aa[grep(linkformat, aa)]
    for (i in 1:length(bb)) # 逐行处理
    {
      bb[i] <- substring(bb[i], regexpr("http", bb[i])[1], regexpr(".jpg\"", bb[i])[1]+3) # 获取链接
    }
    bb <- unique(bb)
    filename <- substring(bb, 47, 50)
    
    for (i in 1:length(bb)) # 逐行处理
    {
      download.file(bb[i], paste(filename[i], i, '.jpg', sep = ""), method = 'curl', quiet = TRUE) # 下载网页
    #   Sys.sleep(5)
    }

 [1]: http://www.biomet.co.at/
 [2]: http://www.biomet.co.at/pictures/
